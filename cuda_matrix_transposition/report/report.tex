\documentclass[]{IEEEconf}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}

\title{\textbf{GPU Computing} \\
    \large Homework 2: Matrix Transposition with CUDA\\
}
\author{Murtas Cristian \\ 248025 \\ cristian.murtas@studenti.unitn.it \\
\underline{\href{https://github.com/SecondarySkyler/gpu-computing/tree/main/matrix_transposition}{GitHub Repository}}
}

\begin{document}
\maketitle
\nocite{*}

\begin{abstract}
This report shows the results obtained by the implementation of different algorithms for matrix transposition
on a GPU using CUDA. The first algorithm is a naive implementation that uses global memory to store the matrix
and performs the transposition. The second algorithm uses shared memory to reduce the number of global memory accesses.
Lastly, the third algorithm uses shared memory and avoid bank conflicts. These algorithms have been tested using different
kernel configurations to find the best parameters. DIRE QUALCOSA SUI RISULTATI.
\end{abstract}
\section{Problem Description}
The goal of this homework is to implement different algorithms to transpose non-symmetric matrices of size $N \times N$.
The transpose of a matrix is an operation that flips the matrix over its diagonal \cite{wiki:transposition}.
The program should use an input parameter to set the size of the matrix, for instance, \textbf{transpose 12} will work on a 
$2^{12} \times 2^{12}$ matrix. 
We are also asked to find the best kernel configuration in terms of grid and block size, and to compare the performance 
of the different algorithms, evaluating the effective bandwidth.
\section{CUDA Algorithms}
\subsection{Naive Transposition}
In the \textit{NaiveTranspose} algorithm, we simply read the element at position $(row, col)$ from the source matrix and write it to the destination matrix at position $(col, row)$.
Note that while the accesses to the $SRC$ matrix are coalesced, the accesses to the $DST$ matrix are not, indeed the writes to $DST$ have a stride equal
to the width of the matrix.
\begin{algorithm}
    \caption{Naive Matrix Transposition}
    \begin{algorithmic}[1]
        \Procedure{NaiveTranspose}{$src, dst, width$}
            \State $row \gets blockIdx.x \times blockDim.x + threadIdx.x$
            \State $col \gets blockIdx.y \times blockDim.y + threadIdx.y$
            \If{$row \le width \; \&\& \; col \le width$}
                \State $dst[row \times width + col] = src[col \times width + row]$
            \EndIf
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\subsection{Transposition with Coalesced Memory}
This algorithm exploits the usage of the shared memory to store a tile of the matrix, avoiding the large strides trough 
the global memory.
\begin{algorithm}
    \caption{Matrix Transpose with Shared Memory}
    \begin{algorithmic}[1]
        \Procedure{Tr\_Coalesced}{$src, dst, width$}
            \State $tile [TILE\_DIM][TILE\_DIM]$
            \State $row \gets blockIdx.x \times TILE\_DIM + threadIdx.x$
            \State $col \gets blockIdx.y \times TILE\_DIM + threadIdx.y$
            \If{$row \le width \; \&\& \; col \le width$} \Comment copy to shared memory
                \State $tile[tIdx.y][tIdx.x] = src[col \times width + row]$
            \EndIf
            \State $\_\_syncthreads()$
            \State $row \gets blockIdx.x \times TILE\_DIM + threadIdx.x$
            \State $col \gets blockIdx.y \times TILE\_DIM + threadIdx.y$
            \If{$row \le width \; \&\& \; col \le width$}
                \State $dst[col \times width + row] = tile[tIdx.y][tIdx.x]$
            \EndIf
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\subsection{Transposition with Coalesced Memory and No Bank Conflicts}
A wrong access pattern to the shared memory can lead to bank conflicts, which can slow down the execution
of the kernel. To avoid this conflict we can pad the shared memory with an additional column.
\begin{algorithm}
    \caption{Matrix Transpose with Shared Memory with no Bank Conflicts}
    \begin{algorithmic}[1]
        \Procedure{Tr\_Coalesced}{$src, dst, width$}
            \State $tile [TILE\_DIM][TILE\_DIM + 1]$
            \State $row \gets blockIdx.x \times TILE\_DIM + threadIdx.x$
            \State $col \gets blockIdx.y \times TILE\_DIM + threadIdx.y$
            \If{$row \le width \; \&\& \; col \le width$} \Comment copy to shared memory
                \State $tile[tIdx.y][tIdx.x] = src[col \times width + row]$
            \EndIf
            \State $\_\_syncthreads()$
            \State $row \gets blockIdx.x \times TILE\_DIM + threadIdx.x$
            \State $col \gets blockIdx.y \times TILE\_DIM + threadIdx.y$
            \If{$row \le width \; \&\& \; col \le width$}
                \State $dst[col \times width + row] = tile[tIdx.y][tIdx.x]$
            \EndIf
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\section{Experimental Setup}
The entire experiment has been conducted on the Marzola cluster of the University of Trento, 
which is equipped with NVIDIA A30 GPUs \cite{nvidia:a30}.
Furthermore, the code has also been tested on a local machine equipped with an NVIDIA RTX 3070 \cite{nvidia:rtx3070}.
For both setup, the information about the GPUs have been retrieved using the \textit{cudaGetDeviceProperties} function \cite{nvidia:cudaDeviceProp}.
\section{Results}
The algorithms have been first tested with a matrix of size $1024 \times 1024$ and a fixed kernel configuration with 4096 blocks each containing 256 threads.
The results, shown in table \ref{tab:bandwidth}, clearly highlight how in this case the access pattern matters. Indeed, having a global coalesced access is a key
point to achieve a good performance.
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \multicolumn{3}{|c|}{\textbf{Effective Bandwidth (GB/s)}} \\
        \hline
        Algorithm & RTX 3070 & A30 \\ \hline
        Naive     & 280.49   & 285.14 \\ \hline
        Coalesced & 372.41   & 523.79 \\ \hline
        Coalesced no bank conflicts & 386.32 & 601.03 \\ \hline
    \end{tabular}
    \caption{Effective Bandwidth of the different algorithms}
    \label{tab:bandwidth}
\end{table}
\section{Conclusions}

\bibliographystyle{plain}
\bibliography{references}
\end{document}